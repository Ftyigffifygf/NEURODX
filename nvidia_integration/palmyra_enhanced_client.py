"""
Enhanced NVIDIA Palmyra-Med-70B client with advanced medical AI capabilities.
Provides medical text analysis, report generation, and diagnostic insights.
"""

import asyncio
import json
import logging
from typing import Dict, List, Optional, Any, AsyncGenerator
from datetime import datetime
import openai
from dataclasses import dataclass
import numpy as np

from src.config.settings import settings
from src.services.security.audit_logger import AuditLogger

logger = logging.getLogger(__name__)


@dataclass
class MedicalInsight:
    """Medical insight generated by Palmyra-Med-70B."""
    insight_type: str
    content: str
    confidence: float
    supporting_evidence: List[str]
    clinical_relevance: str
    timestamp: datetime


@dataclass
class DiagnosticReport:
    """Comprehensive diagnostic report."""
    patient_id: str
    report_id: str
    summary: str
    findings: List[str]
    recommendations: List[str]
    risk_assessment: Dict[str, float]
    follow_up_suggestions: List[str]
    generated_at: datetime
    model_version: str


class PalmyraMedEnhancedClient:
    """
    Enhanced client for NVIDIA Palmyra-Med-70B with advanced medical AI capabilities.
    
    Features:
    - Medical text analysis and interpretation
    - Comprehensive diagnostic report generation
    - Multi-modal data integration insights
    - Streaming responses for real-time analysis
    - Clinical decision support
    - Risk assessment and stratification
    """
    
    def __init__(self, audit_logger: Optional[AuditLogger] = None):
        """Initialize enhanced Palmyra client."""
        self.audit_logger = audit_logger or AuditLogger()
        
        # Initialize OpenAI client with NVIDIA endpoints
        self.clients = []
        self.current_client_index = 0
        
        # Set up multiple API keys for load balancing and failover
        api_keys = [settings.nvidia.palmyra_api_key]
        
        # Try to get additional API keys if they exist
        try:
            if hasattr(settings.nvidia, 'palmyra_api_key_2') and settings.nvidia.palmyra_api_key_2:
                api_keys.append(settings.nvidia.palmyra_api_key_2)
        except AttributeError:
            pass
            
        try:
            if hasattr(settings.nvidia, 'palmyra_api_key_3') and settings.nvidia.palmyra_api_key_3:
                api_keys.append(settings.nvidia.palmyra_api_key_3)
        except AttributeError:
            pass
        
        for i, api_key in enumerate(api_keys):
            if api_key:
                client = openai.OpenAI(
                    api_key=api_key,
                    base_url=settings.nvidia.palmyra_base_url
                )
                self.clients.append(client)
                logger.info(f"Initialized Palmyra client {i+1}")
        
        if not self.clients:
            raise ValueError("No valid Palmyra API keys configured")
        
        # Model configuration
        self.model_name = "nvidia/palmyra-med-70b"
        self.max_tokens = 4096
        self.temperature = 0.1  # Low temperature for medical accuracy
        
        # Medical specialization prompts
        self.system_prompts = {
            "diagnostic_analysis": """You are an expert medical AI assistant specializing in neurodegenerative diseases. 
            Analyze the provided medical data and imaging results to provide accurate diagnostic insights. 
            Focus on Alzheimer's disease, Parkinson's disease, and mild cognitive impairment. 
            Provide evidence-based analysis with confidence scores.""",
            
            "report_generation": """You are a medical report writer specializing in neurological assessments. 
            Generate comprehensive, professional medical reports based on imaging data, wearable sensor data, 
            and diagnostic results. Follow standard medical reporting formats and include clear recommendations.""",
            
            "risk_assessment": """You are a clinical risk assessment specialist. 
            Analyze patient data to identify risk factors for neurodegenerative diseases. 
            Provide stratified risk assessments with actionable recommendations for monitoring and intervention.""",
            
            "longitudinal_analysis": """You are a specialist in longitudinal medical data analysis. 
            Compare current results with historical data to identify disease progression patterns. 
            Highlight significant changes and provide insights into disease trajectory."""
        }
    
    def _get_next_client(self) -> openai.OpenAI:
        """Get next available client for load balancing."""
        client = self.clients[self.current_client_index]
        self.current_client_index = (self.current_client_index + 1) % len(self.clients)
        return client
    
    async def analyze_diagnostic_data(self, 
                                    patient_data: Dict[str, Any],
                                    imaging_results: Dict[str, Any],
                                    wearable_data: Dict[str, Any]) -> MedicalInsight:
        """
        Analyze multi-modal diagnostic data to generate medical insights.
        
        Args:
            patient_data: Patient demographics and history
            imaging_results: Brain imaging analysis results
            wearable_data: Wearable sensor analysis results
            
        Returns:
            Medical insight with diagnostic analysis
        """
        try:
            # Prepare structured input for the model
            analysis_prompt = self._create_diagnostic_prompt(
                patient_data, imaging_results, wearable_data
            )
            
            client = self._get_next_client()
            
            response = await asyncio.to_thread(
                client.chat.completions.create,
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompts["diagnostic_analysis"]},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            
            # Parse response
            analysis_content = response.choices[0].message.content
            
            # Extract structured insights
            insight = self._parse_diagnostic_response(analysis_content)
            
            # Audit log
            self.audit_logger.log_ai_interaction(
                service="palmyra_med",
                action="diagnostic_analysis",
                patient_id=patient_data.get("patient_id"),
                input_tokens=response.usage.prompt_tokens,
                output_tokens=response.usage.completion_tokens,
                model_version=self.model_name
            )
            
            return insight
            
        except Exception as e:
            logger.error(f"Diagnostic analysis failed: {e}")
            self.audit_logger.log_ai_error(
                service="palmyra_med",
                action="diagnostic_analysis",
                error=str(e),
                patient_id=patient_data.get("patient_id")
            )
            raise
    
    async def generate_comprehensive_report(self,
                                          patient_data: Dict[str, Any],
                                          diagnostic_results: Dict[str, Any],
                                          medical_history: List[Dict[str, Any]]) -> DiagnosticReport:
        """
        Generate comprehensive diagnostic report.
        
        Args:
            patient_data: Patient information
            diagnostic_results: All diagnostic test results
            medical_history: Historical medical data
            
        Returns:
            Comprehensive diagnostic report
        """
        try:
            # Create report generation prompt
            report_prompt = self._create_report_prompt(
                patient_data, diagnostic_results, medical_history
            )
            
            client = self._get_next_client()
            
            response = await asyncio.to_thread(
                client.chat.completions.create,
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompts["report_generation"]},
                    {"role": "user", "content": report_prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            
            # Parse and structure the report
            report_content = response.choices[0].message.content
            report = self._parse_report_response(report_content, patient_data)
            
            # Audit log
            self.audit_logger.log_ai_interaction(
                service="palmyra_med",
                action="report_generation",
                patient_id=patient_data.get("patient_id"),
                input_tokens=response.usage.prompt_tokens,
                output_tokens=response.usage.completion_tokens,
                model_version=self.model_name
            )
            
            return report
            
        except Exception as e:
            logger.error(f"Report generation failed: {e}")
            raise
    
    async def assess_disease_risk(self,
                                patient_data: Dict[str, Any],
                                biomarkers: Dict[str, float],
                                lifestyle_factors: Dict[str, Any]) -> Dict[str, Any]:
        """
        Assess risk for neurodegenerative diseases.
        
        Args:
            patient_data: Patient demographics and history
            biomarkers: Biomarker measurements
            lifestyle_factors: Lifestyle and environmental factors
            
        Returns:
            Risk assessment with scores and recommendations
        """
        try:
            risk_prompt = self._create_risk_assessment_prompt(
                patient_data, biomarkers, lifestyle_factors
            )
            
            client = self._get_next_client()
            
            response = await asyncio.to_thread(
                client.chat.completions.create,
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompts["risk_assessment"]},
                    {"role": "user", "content": risk_prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            
            risk_assessment = self._parse_risk_response(response.choices[0].message.content)
            
            return risk_assessment
            
        except Exception as e:
            logger.error(f"Risk assessment failed: {e}")
            raise
    
    async def analyze_longitudinal_progression(self,
                                             patient_id: str,
                                             historical_data: List[Dict[str, Any]],
                                             current_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze disease progression over time.
        
        Args:
            patient_id: Patient identifier
            historical_data: Previous diagnostic results
            current_data: Current diagnostic results
            
        Returns:
            Longitudinal analysis with progression insights
        """
        try:
            progression_prompt = self._create_longitudinal_prompt(
                historical_data, current_data
            )
            
            client = self._get_next_client()
            
            response = await asyncio.to_thread(
                client.chat.completions.create,
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompts["longitudinal_analysis"]},
                    {"role": "user", "content": progression_prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            
            progression_analysis = self._parse_longitudinal_response(
                response.choices[0].message.content
            )
            
            return progression_analysis
            
        except Exception as e:
            logger.error(f"Longitudinal analysis failed: {e}")
            raise
    
    async def stream_diagnostic_insights(self,
                                       patient_data: Dict[str, Any],
                                       diagnostic_data: Dict[str, Any]) -> AsyncGenerator[str, None]:
        """
        Stream diagnostic insights in real-time.
        
        Args:
            patient_data: Patient information
            diagnostic_data: Diagnostic test results
            
        Yields:
            Streaming diagnostic insights
        """
        try:
            prompt = self._create_diagnostic_prompt(patient_data, diagnostic_data, {})
            
            client = self._get_next_client()
            
            stream = await asyncio.to_thread(
                client.chat.completions.create,
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompts["diagnostic_analysis"]},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"Streaming insights failed: {e}")
            raise
    
    def _create_diagnostic_prompt(self,
                                patient_data: Dict[str, Any],
                                imaging_results: Dict[str, Any],
                                wearable_data: Dict[str, Any]) -> str:
        """Create structured prompt for diagnostic analysis."""
        
        prompt = f"""
        Please analyze the following multi-modal medical data for neurodegenerative disease assessment:

        PATIENT INFORMATION:
        - Age: {patient_data.get('age', 'Unknown')}
        - Gender: {patient_data.get('gender', 'Unknown')}
        - Medical History: {patient_data.get('medical_history', [])}
        
        BRAIN IMAGING RESULTS:
        - Modality: {imaging_results.get('modality', 'Unknown')}
        - Segmentation Results: {imaging_results.get('segmentation_summary', {})}
        - Classification Probabilities: {imaging_results.get('classification_probabilities', {})}
        - Volumetric Measurements: {imaging_results.get('volumetric_data', {})}
        
        WEARABLE SENSOR DATA:
        - EEG Features: {wearable_data.get('eeg_features', {})}
        - Cognitive Metrics: {wearable_data.get('cognitive_metrics', {})}
        - Sleep Patterns: {wearable_data.get('sleep_data', {})}
        - Gait Analysis: {wearable_data.get('gait_features', {})}
        
        Please provide:
        1. Diagnostic assessment with confidence level
        2. Key findings and their clinical significance
        3. Risk stratification for neurodegenerative diseases
        4. Recommended follow-up actions
        5. Supporting evidence for your analysis
        
        Format your response as structured JSON with clear sections.
        """
        
        return prompt
    
    def _create_report_prompt(self,
                            patient_data: Dict[str, Any],
                            diagnostic_results: Dict[str, Any],
                            medical_history: List[Dict[str, Any]]) -> str:
        """Create prompt for comprehensive report generation."""
        
        prompt = f"""
        Generate a comprehensive neurological diagnostic report for the following patient:

        PATIENT DEMOGRAPHICS:
        {json.dumps(patient_data, indent=2)}
        
        DIAGNOSTIC RESULTS:
        {json.dumps(diagnostic_results, indent=2)}
        
        MEDICAL HISTORY:
        {json.dumps(medical_history, indent=2)}
        
        Please generate a professional medical report including:
        1. Executive Summary
        2. Clinical Findings
        3. Diagnostic Impression
        4. Risk Assessment
        5. Treatment Recommendations
        6. Follow-up Plan
        7. Prognosis
        
        Use standard medical terminology and format the report professionally.
        """
        
        return prompt
    
    def _create_risk_assessment_prompt(self,
                                     patient_data: Dict[str, Any],
                                     biomarkers: Dict[str, float],
                                     lifestyle_factors: Dict[str, Any]) -> str:
        """Create prompt for risk assessment."""
        
        prompt = f"""
        Assess the risk for neurodegenerative diseases based on:

        PATIENT DATA:
        {json.dumps(patient_data, indent=2)}
        
        BIOMARKERS:
        {json.dumps(biomarkers, indent=2)}
        
        LIFESTYLE FACTORS:
        {json.dumps(lifestyle_factors, indent=2)}
        
        Provide risk scores (0-100) for:
        - Alzheimer's Disease
        - Parkinson's Disease
        - Mild Cognitive Impairment
        - Vascular Dementia
        
        Include risk factors, protective factors, and recommendations.
        """
        
        return prompt
    
    def _create_longitudinal_prompt(self,
                                  historical_data: List[Dict[str, Any]],
                                  current_data: Dict[str, Any]) -> str:
        """Create prompt for longitudinal analysis."""
        
        prompt = f"""
        Analyze disease progression based on longitudinal data:

        HISTORICAL DATA:
        {json.dumps(historical_data, indent=2)}
        
        CURRENT DATA:
        {json.dumps(current_data, indent=2)}
        
        Analyze:
        1. Disease progression rate
        2. Significant changes over time
        3. Treatment response
        4. Prognosis update
        5. Recommended interventions
        """
        
        return prompt
    
    def _parse_diagnostic_response(self, response_content: str) -> MedicalInsight:
        """Parse diagnostic analysis response."""
        try:
            # Try to parse as JSON first
            if response_content.strip().startswith('{'):
                data = json.loads(response_content)
                return MedicalInsight(
                    insight_type="diagnostic_analysis",
                    content=data.get("assessment", response_content),
                    confidence=data.get("confidence", 0.8),
                    supporting_evidence=data.get("evidence", []),
                    clinical_relevance=data.get("clinical_relevance", "High"),
                    timestamp=datetime.now()
                )
            else:
                # Parse unstructured response
                return MedicalInsight(
                    insight_type="diagnostic_analysis",
                    content=response_content,
                    confidence=0.8,
                    supporting_evidence=[],
                    clinical_relevance="High",
                    timestamp=datetime.now()
                )
        except json.JSONDecodeError:
            return MedicalInsight(
                insight_type="diagnostic_analysis",
                content=response_content,
                confidence=0.8,
                supporting_evidence=[],
                clinical_relevance="High",
                timestamp=datetime.now()
            )
    
    def _parse_report_response(self, response_content: str, patient_data: Dict[str, Any]) -> DiagnosticReport:
        """Parse comprehensive report response."""
        return DiagnosticReport(
            patient_id=patient_data.get("patient_id", "Unknown"),
            report_id=f"RPT_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            summary=self._extract_section(response_content, "Executive Summary"),
            findings=self._extract_list_section(response_content, "Clinical Findings"),
            recommendations=self._extract_list_section(response_content, "Treatment Recommendations"),
            risk_assessment=self._extract_risk_scores(response_content),
            follow_up_suggestions=self._extract_list_section(response_content, "Follow-up Plan"),
            generated_at=datetime.now(),
            model_version=self.model_name
        )
    
    def _parse_risk_response(self, response_content: str) -> Dict[str, Any]:
        """Parse risk assessment response."""
        # Extract risk scores and factors from response
        return {
            "risk_scores": self._extract_risk_scores(response_content),
            "risk_factors": self._extract_list_section(response_content, "Risk Factors"),
            "protective_factors": self._extract_list_section(response_content, "Protective Factors"),
            "recommendations": self._extract_list_section(response_content, "Recommendations")
        }
    
    def _parse_longitudinal_response(self, response_content: str) -> Dict[str, Any]:
        """Parse longitudinal analysis response."""
        return {
            "progression_rate": self._extract_section(response_content, "Progression Rate"),
            "significant_changes": self._extract_list_section(response_content, "Significant Changes"),
            "treatment_response": self._extract_section(response_content, "Treatment Response"),
            "prognosis_update": self._extract_section(response_content, "Prognosis"),
            "interventions": self._extract_list_section(response_content, "Interventions")
        }
    
    def _extract_section(self, content: str, section_name: str) -> str:
        """Extract a specific section from the response."""
        lines = content.split('\n')
        in_section = False
        section_content = []
        
        for line in lines:
            if section_name.lower() in line.lower():
                in_section = True
                continue
            elif in_section and line.strip() and line[0].isupper():
                # New section started
                break
            elif in_section:
                section_content.append(line.strip())
        
        return '\n'.join(section_content).strip()
    
    def _extract_list_section(self, content: str, section_name: str) -> List[str]:
        """Extract a list section from the response."""
        section_text = self._extract_section(content, section_name)
        if not section_text:
            return []
        
        # Extract bullet points or numbered items
        items = []
        for line in section_text.split('\n'):
            line = line.strip()
            if line.startswith(('-', '•', '*')) or (line and line[0].isdigit()):
                # Remove bullet point or number
                item = line.lstrip('-•*0123456789. ').strip()
                if item:
                    items.append(item)
        
        return items
    
    def _extract_risk_scores(self, content: str) -> Dict[str, float]:
        """Extract risk scores from the response."""
        risk_scores = {}
        diseases = ["alzheimer", "parkinson", "cognitive impairment", "dementia"]
        
        for disease in diseases:
            # Look for patterns like "Alzheimer's Disease: 75%" or "Risk: 0.75"
            import re
            pattern = rf"{disease}.*?(\d+(?:\.\d+)?)"
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                score = float(match.group(1))
                # Normalize to 0-1 range if needed
                if score > 1:
                    score = score / 100
                risk_scores[disease] = score
        
        return risk_scores
    
    async def get_model_health(self) -> Dict[str, Any]:
        """Check model health and availability."""
        health_status = {
            "available_clients": len(self.clients),
            "model_name": self.model_name,
            "last_check": datetime.now(),
            "client_status": []
        }
        
        for i, client in enumerate(self.clients):
            try:
                # Simple health check
                response = await asyncio.to_thread(
                    client.chat.completions.create,
                    model=self.model_name,
                    messages=[{"role": "user", "content": "Health check"}],
                    max_tokens=10
                )
                health_status["client_status"].append({
                    "client_id": i,
                    "status": "healthy",
                    "response_time": "< 1s"
                })
            except Exception as e:
                health_status["client_status"].append({
                    "client_id": i,
                    "status": "unhealthy",
                    "error": str(e)
                })
        
        return health_status